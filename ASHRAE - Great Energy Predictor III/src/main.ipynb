{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature import *\n",
    "from model import *\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import gc\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evel_model(eval_df):\n",
    "    eval_df['y'] = np.log(eval_df['y'] + 1)\n",
    "    eval_df['p'] = np.log(eval_df['p'] + 1)\n",
    "    return np.sqrt(np.sum((eval_df['p'] - eval_df['y']) * (eval_df['p'] - eval_df['y'])) / eval_df.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline = False\n",
    "train = pd.read_pickle(INPUT_PATH + 'train.pk')\n",
    "if offline == True:\n",
    "    train_model = train[train['month']<12]\n",
    "    test_model = train[train['month']==12]\n",
    "else:\n",
    "    test = pd.read_pickle(INPUT_PATH + 'test.pk')\n",
    "    train_model = train\n",
    "    test_model = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use memory 1709.9349975585938 (41697600, 19)\n",
      "use memory 776.9253978729248 (19869886, 19)\n"
     ]
    }
   ],
   "source": [
    "print(f'use memory {test.memory_usage().sum() / 1024**2}', test.shape)\n",
    "print(f'use memory {train.memory_usage().sum() / 1024**2}', train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'n_estimators': 1000,\n",
    "    'metric': 'mae',\n",
    "    'learning_rate': 0.1,\n",
    "    'min_child_samples': 5,\n",
    "    'min_child_weight': 0.01,\n",
    "    'subsample_freq': 1,\n",
    "    'num_leaves': 31,\n",
    "    'max_depth': -1,\n",
    "    'subsample': 0.6,\n",
    "    'colsample_bytree': 0.6,\n",
    "    'reg_alpha': 0,\n",
    "    'reg_lambda': 5,\n",
    "    'verbose': -1,\n",
    "    'random_state': 4590,\n",
    "    'n_jobs': 6,\n",
    "\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training......fold 0\n"
     ]
    }
   ],
   "source": [
    "numerical_features = ['year', 'month', 'hour',\n",
    "                      'square_feet', 'year_built', 'floor_count',\n",
    "                      'air_temperature', 'cloud_coverage', 'dew_temperature', 'precip_depth_1_hr', 'sea_level_pressure', 'wind_direction', 'wind_speed'\n",
    "                      ]\n",
    "category_features = ['building_id', 'site_id', 'meter', 'primary_use'\n",
    "                      ]\n",
    "label_name = \"meter_reading\"\n",
    "train_model.reset_index(inplace=True,drop=True)\n",
    "test_model.reset_index(inplace=True,drop=True)\n",
    "features = category_features + numerical_features\n",
    "train_x = train[features]\n",
    "train_y = train[label_name]\n",
    "test_x = test[features]\n",
    "\n",
    "n_fold = 5\n",
    "count_fold = 0\n",
    "preds_list = list()\n",
    "oof = np.zeros(train_x.shape[0])\n",
    "kfolder = KFold(n_splits=n_fold, shuffle=True, random_state=2019)\n",
    "kfold = kfolder.split(train_x, train_y)\n",
    "for train_index, vali_index in kfold:\n",
    "    print(\"training......fold\",count_fold)\n",
    "    count_fold = count_fold + 1\n",
    "    k_x_train = train_x.loc[train_index]\n",
    "    k_y_train = train_y.loc[train_index]\n",
    "    k_x_vali = train_x.loc[vali_index]\n",
    "    k_y_vali = train_y.loc[vali_index]\n",
    "\n",
    "    dtrain = lgb.Dataset(k_x_train, k_y_train)\n",
    "    dvalid = lgb.Dataset(k_x_vali, k_y_vali, reference=dtrain)\n",
    "    lgb_model = lgb.LGBMRegressor(**lgb_params)\n",
    "    if 'sample_weight' in train.columns:\n",
    "        lgb_model = lgb_model.fit(k_x_train, k_y_train, eval_set=[(k_x_vali, k_y_vali)],\n",
    "                                  early_stopping_rounds=200, verbose=False, eval_metric=\"mae\",\n",
    "                                  sample_weight=train.loc[train_index]['sample_weight'],\n",
    "                                  categorical_feature=category_features\n",
    "                                  )\n",
    "    else:\n",
    "        lgb_model = lgb_model.fit(k_x_train, k_y_train, eval_set=[(k_x_vali, k_y_vali)],\n",
    "                                  early_stopping_rounds=200, verbose=False, eval_metric=\"mae\",\n",
    "                                  categorical_feature=category_features)\n",
    "    k_pred = lgb_model.predict(k_x_vali, num_iteration=lgb_model.best_iteration_)\n",
    "    pred = lgb_model.predict(test_x, num_iteration=lgb_model.best_iteration_)\n",
    "\n",
    "    preds_list.append(pred)\n",
    "    oof[vali_index] = k_pred\n",
    "preds_columns = ['preds_{id}'.format(id=i) for i in range(n_fold)]\n",
    "preds_df = pd.DataFrame(data=preds_list)\n",
    "preds_df = preds_df.T\n",
    "preds_df.columns = preds_columns\n",
    "preds = list(preds_df.mean(axis=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if offline:\n",
    "    eval_df = pd.DataFrame()\n",
    "    eval_df['y'] = test_model['meter_reading']\n",
    "    eval_df['p'] = pred\n",
    "    eval_df['y'] = np.expm1(eval_df['y'])\n",
    "    eval_df['p'] = np.expm1(eval_df['p'])\n",
    "    score = evel_model(eval_df)\n",
    "    print(f'test-score:{score}')\n",
    "else:\n",
    "    submit_df = pd.DataFrame()\n",
    "    submit_df['row_id'] = test_model['row_id'].astype(np.int32)\n",
    "    submit_df['meter_reading'] = pred\n",
    "    submit_df['meter_reading'] = np.expm1(submit_df['meter_reading'])\n",
    "    submit_df.loc[submit_df['meter_reading'] < 0, 'meter_reading'] = 0\n",
    "    submit_df['meter_reading'] = submit_df['meter_reading'].round(4)\n",
    "\n",
    "    submit_df.to_csv(SUBMIT_PATH + 'submit1104_1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
